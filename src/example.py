# -*- coding: utf-8 -*-
"""YT Qwen3_VL_Embedding_Multimodal_RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R1WEX2KJ5for89vsNq00AR6RzhrdTHYw

# üåü Qwen3-VL-Embedding-2B: Multimodal RAG Demo

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/)

https://youtu.be/fY3-YeveBgA


## üìã Model Highlights
- **2B parameters** - runs on free Colab GPU!
- **Multimodal**: text, images, screenshots, videos
- **2048-dim embeddings** with MRL support (64-2048)
- **30+ languages** supported
- **32K context length**

---

## üõ†Ô∏è Part 1: Setup & Installation
"""

import torch
import numpy as np
from PIL import Image
import requests
from io import BytesIO
from typing import List, Dict, Any, Optional, Union
import torch.nn.functional as F
from transformers import Qwen3VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
import warnings
warnings.filterwarnings('ignore')

print(f"üî• PyTorch version: {torch.__version__}")
print(f"üñ•Ô∏è CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"üéÆ GPU: {torch.cuda.get_device_name(0)}")
    print(f"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

"""## üß† Part 2: Qwen3-VL Embedder Implementation

We'll create a clean wrapper class for the embedding model that handles:
- Text-only inputs
- Image-only inputs
- Mixed text+image inputs
"""

import torch
import torch.nn.functional as F
import unicodedata
import numpy as np
import logging

from PIL import Image
from dataclasses import dataclass
from typing import Optional, List, Union, Dict, Any
from transformers.models.qwen3_vl.modeling_qwen3_vl import Qwen3VLPreTrainedModel, Qwen3VLModel, Qwen3VLConfig
from transformers.models.qwen3_vl.processing_qwen3_vl import Qwen3VLProcessor
from transformers.modeling_outputs import ModelOutput
from transformers.processing_utils import Unpack
from transformers.utils import TransformersKwargs
from transformers.cache_utils import Cache
from transformers.utils.generic import check_model_inputs
from qwen_vl_utils.vision_process import process_vision_info

logger = logging.getLogger(__name__)

# Constants for configuration
MAX_LENGTH = 8192
IMAGE_BASE_FACTOR = 16
IMAGE_FACTOR = IMAGE_BASE_FACTOR * 2
MIN_PIXELS = 4 * IMAGE_FACTOR * IMAGE_FACTOR
MAX_PIXELS = 1800 * IMAGE_FACTOR * IMAGE_FACTOR
FPS = 1
MAX_FRAMES = 64
FRAME_MAX_PIXELS = 768 * IMAGE_FACTOR * IMAGE_FACTOR
MAX_TOTAL_PIXELS = 10 * FRAME_MAX_PIXELS
PAD_TOKEN = "<|endoftext|>"

# Define output structure for embeddings
@dataclass
class Qwen3VLForEmbeddingOutput(ModelOutput):
    last_hidden_state: Optional[torch.FloatTensor] = None
    attention_mask: Optional[torch.Tensor] = None

# Define model class to compute embeddings
class Qwen3VLForEmbedding(Qwen3VLPreTrainedModel):
    _checkpoint_conversion_mapping = {}
    accepts_loss_kwargs = False
    config: Qwen3VLConfig

    def __init__(self, config):
        super().__init__(config)
        self.model = Qwen3VLModel(config)
        self.post_init()

    def get_input_embeddings(self):
        return self.model.get_input_embeddings()

    def set_input_embeddings(self, value):
        self.model.set_input_embeddings(value)

    def set_decoder(self, decoder):
        self.model.set_decoder(decoder)

    def get_decoder(self):
        return self.model.get_decoder()

    # Extract video features from model
    def get_video_features(self, pixel_values_videos: torch.FloatTensor,
                           video_grid_thw: Optional[torch.LongTensor] = None):
        return self.model.get_video_features(pixel_values_videos, video_grid_thw)

    # Extract image features from model
    def get_image_features(self, pixel_values: torch.FloatTensor,
                           image_grid_thw: Optional[torch.LongTensor] = None):
        return self.model.get_image_features(pixel_values, image_grid_thw)

    # Make modules accessible through properties
    @property
    def language_model(self):
        return self.model.language_model

    @property
    def visual(self):
        return self.model.visual

    # Forward pass through model with input parameters
    # @check_model_inputs
    def forward(self,
                input_ids: torch.LongTensor = None,
                attention_mask: Optional[torch.Tensor] = None,
                position_ids: Optional[torch.LongTensor] = None,
                past_key_values: Optional[Cache] = None,
                inputs_embeds: Optional[torch.FloatTensor] = None,
                pixel_values: Optional[torch.Tensor] = None,
                pixel_values_videos: Optional[torch.FloatTensor] = None,
                image_grid_thw: Optional[torch.LongTensor] = None,
                video_grid_thw: Optional[torch.LongTensor] = None,
                cache_position: Optional[torch.LongTensor] = None,
                logits_to_keep: Union[int, torch.Tensor] = 0,
                **kwargs: Unpack[TransformersKwargs],
    ) -> Union[tuple, Qwen3VLForEmbeddingOutput]:
        # Pass inputs through the model
        outputs = self.model(
            input_ids=input_ids,
            pixel_values=pixel_values,
            pixel_values_videos=pixel_values_videos,
            image_grid_thw=image_grid_thw,
            video_grid_thw=video_grid_thw,
            position_ids=position_ids,
            attention_mask=attention_mask,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            cache_position=cache_position,
            **kwargs,
        )
        # Return the model output
        return Qwen3VLForEmbeddingOutput(
            last_hidden_state=outputs.last_hidden_state,
            attention_mask=attention_mask,
        )

def sample_frames(frames: List[Union[str, Image.Image]], num_segments: int, max_segments: int) -> List[str]:
    duration = len(frames)
    frame_id_array = np.linspace(0, duration - 1, num_segments, dtype=int)
    frame_id_list = frame_id_array.tolist()
    last_frame_id = frame_id_list[-1]

    # Create a list of sampled frames
    sampled_frames = []
    for frame_idx in frame_id_list:
        try:
            sampled_frames.append(frames[frame_idx])
        except:
            break
    # Ensure the sampled list meets the required segment count
    while len(sampled_frames) < num_segments:
        sampled_frames.append(frames[last_frame_id])
    return sampled_frames[:max_segments]

# Define embedder class for processing inputs and generating embeddings
class Qwen3VLEmbedder():
    def __init__(
        self,
        model_name_or_path: str,
        max_length: int = MAX_LENGTH,
        min_pixels: int = MIN_PIXELS,
        max_pixels: int = MAX_PIXELS,
        total_pixels: int = MAX_TOTAL_PIXELS,
        fps: float = FPS,
        num_frames: int = MAX_FRAMES,
        max_frames: int = MAX_FRAMES,
        default_instruction: str = "Represent the user's input.",
        **kwargs
    ):
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        self.max_length = max_length
        self.min_pixels = min_pixels
        self.max_pixels = max_pixels
        self.total_pixels = total_pixels
        self.fps = fps
        self.num_frames = num_frames
        self.max_frames = max_frames

        self.default_instruction = default_instruction

        self.model = Qwen3VLForEmbedding.from_pretrained(
            model_name_or_path, trust_remote_code=True, **kwargs
        ).to(device)
        self.processor = Qwen3VLProcessor.from_pretrained(
            model_name_or_path, padding_side='right'
        )
        self.model.eval()

    @torch.no_grad()
    def forward(self, inputs: Dict[str, Any]) -> Dict[str, torch.Tensor]:
        outputs = self.model(**inputs)
        return {
            'last_hidden_state': outputs.last_hidden_state,
            'attention_mask': inputs.get('attention_mask')
        }

    # Truncate token sequence to a specified max length
    def _truncate_tokens(self, token_ids: List[int], max_length: int) -> List[int]:
        if len(token_ids) <= max_length:
            return token_ids

        special_token_ids = set(self.processor.tokenizer.all_special_ids)
        num_special = sum(1 for token_idx in token_ids if token_idx in special_token_ids)
        num_non_special_to_keep = max_length - num_special

        final_token_ids = []
        non_special_kept_count = 0
        # Ensure retention of special tokens while truncating the rest
        for token_idx in token_ids:
            if token_idx in special_token_ids:
                final_token_ids.append(token_idx)
            elif non_special_kept_count < num_non_special_to_keep:
                final_token_ids.append(token_idx)
                non_special_kept_count += 1
        return final_token_ids

    # Format input based on provided text, image, video, and instruction
    def format_model_input(
        self, text: Optional[str] = None,
        image: Optional[Union[str, Image.Image]] = None,
        video: Optional[Union[str, List[Union[str, Image.Image]]]] = None,
        instruction: Optional[str] = None,
        fps: Optional[float] = None,
        max_frames: Optional[int] = None
    ) -> List[Dict]:

        # Ensure instruction ends with punctuation
        if instruction:
            instruction = instruction.strip()
            if instruction and not unicodedata.category(instruction[-1]).startswith('P'):
                instruction = instruction + '.'

        # Initialize conversation with system prompts
        content = []
        conversation = [
            {"role": "system", "content": [{"type": "text", "text": instruction or self.default_instruction}]},
            {"role": "user", "content": content}
        ]

        # Add text, image, or video content to conversation
        if not text and not image and not video:
            content.append({'type': 'text', 'text': "NULL"})
            return conversation

        if video:
            video_content = None
            video_kwargs = { 'total_pixels': self.total_pixels }
            if isinstance(video, list):
                video_content = video
                if self.num_frames is not None or self.max_frames is not None:
                    video_content = sample_frames(video_content, self.num_frames, self.max_frames)
                video_content = [
                    ('file://' + ele if isinstance(ele, str) else ele)
                    for ele in video_content
                ]
            elif isinstance(video, str):
                video_content = video if video.startswith(('http://', 'https://')) else 'file://' + video
                video_kwargs = {'fps': fps or self.fps, 'max_frames': max_frames or self.max_frames,}
            else:
                raise TypeError(f"Unrecognized video type: {type(video)}")

            # Add video input details to content
            if video_content:
                content.append({
                    'type': 'video', 'video': video_content,
                    **video_kwargs
                })

        if image:
            image_content = None
            if isinstance(image, Image.Image):
                image_content = image
            elif isinstance(image, str):
                image_content = image if image.startswith(('http', 'oss')) else 'file://' + image
            else:
                raise TypeError(f"Unrecognized image type: {type(image)}")

            # Add image input details to content
            if image_content:
                content.append({
                    'type': 'image', 'image': image_content,
                    "min_pixels": self.min_pixels,
                    "max_pixels": self.max_pixels
                })

        if text:
            content.append({'type': 'text', 'text': text})

        return conversation

    # Preprocess input conversations for model consumption
    def _preprocess_inputs(self, conversations: List[List[Dict]]) -> Dict[str, torch.Tensor]:
        text = self.processor.apply_chat_template(
            conversations, add_generation_prompt=True, tokenize=False
        )

        try:
            images, video_inputs, video_kwargs = process_vision_info(
                conversations, image_patch_size=16,
                return_video_metadata=True, return_video_kwargs=True
            )
        except Exception as e:
            logger.error(f"Error in processing vision info: {e}")
            images = None
            video_inputs = None
            video_kwargs = {'do_sample_frames': False}
            text = self.processor.apply_chat_template(
                [{'role': 'user', 'content': [{'type': 'text', 'text': 'NULL'}]}],
                add_generation_prompt=True, tokenize=False
            )

        if video_inputs is not None:
            videos, video_metadata = zip(*video_inputs)
            videos = list(videos)
            video_metadata = list(video_metadata)
        else:
            videos, video_metadata = None, None

        inputs = self.processor(
            text=text, images=images, videos=videos, video_metadata=video_metadata, truncation=True,
            max_length=self.max_length, padding=True, do_resize=False, return_tensors='pt',
            **video_kwargs
        )
        return inputs

    # Pool the last hidden state by attention mask for embeddings
    @staticmethod
    def _pooling_last(hidden_state: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
        flipped_tensor = attention_mask.flip(dims=[1])
        last_one_positions = flipped_tensor.argmax(dim=1)
        col = attention_mask.shape[1] - last_one_positions - 1
        row = torch.arange(hidden_state.shape[0], device=hidden_state.device)
        return hidden_state[row, col]

    # Process inputs to generate normalized embeddings
    def process(self, inputs: List[Dict[str, Any]], normalize: bool = True) -> tuple:
        conversations = [self.format_model_input(
            text=ele.get('text'),
            image=ele.get('image'),
            video=ele.get('video'),
            instruction=ele.get('instruction'),
            fps=ele.get('fps'),
            max_frames=ele.get('max_frames')
        ) for ele in inputs]

        processed_inputs = self._preprocess_inputs(conversations)
        processed_inputs = {k: v.to(self.model.device) for k, v in processed_inputs.items()}

        outputs = self.forward(processed_inputs)
        embeddings = self._pooling_last(outputs['last_hidden_state'], outputs['attention_mask'])

        # Normalize the embeddings if specified
        if normalize:
            embeddings = F.normalize(embeddings, p=2, dim=-1)

        return embeddings



# =============================================================================
# Example usage
# =============================================================================
# Example 1: Load model automatically
print("=" * 60)
print("Example 1: Auto-load model")
print("=" * 60)

embedder = Qwen3VLEmbedderV2(
    model_name_or_path="Qwen/Qwen3-VL-Embedding-2B",
    torch_dtype=torch.bfloat16
)

# Test inputs
inputs = [
    {"text": "A woman playing with her dog on a beach at sunset."},
    {"text": "City skyline at night with bright lights."},
]

embeddings = embedder.process(inputs)
print(f"Embeddings shape: {embeddings.shape}")
print(f"Similarity: {embeddings[0] @ embeddings[1]:.4f}")

# Example 2: Use pre-loaded model
# print("\n" + "=" * 60)
# print("Example 2: Pre-loaded model")
# print("=" * 60)

# Simulate pre-loaded model
# model = embedder.model
# processor = embedder.processor

# embedder2 = Qwen3VLEmbedderV2(
#     model=model,
#     processor=processor
# )

# embeddings2 = embedder2.process(inputs)
# print(f"Embeddings shape: {embeddings2.shape}")
# print(f"Same results: {np.allclose(embeddings, embeddings2)}")



"""## üì¶ Part 3: Load the Model

This will download the model (~4.3GB) if not cached. On a free Colab T4 GPU, this takes about 2-3 minutes.
"""

# # Initialize the embedder
# embedder = Qwen3VLEmbedder(
#     model_name_or_path="Qwen/Qwen3-VL-Embedding-2B",
#     torch_dtype=torch.bfloat16,
# )

# Specify the model path
# model_name_or_path = "Qwen/Qwen3-VL-Embedding-2B"

# # Initialize the Qwen3VLEmbedder model
# embedder = Qwen3VLEmbedder(model_name_or_path=model_name_or_path)

# We recommend enabling flash_attention_2 for better acceleration and memory saving,
# model = Qwen3VLEmbedder(model_name_or_path=model_name_or_path, torch_dtype=torch.float16, attn_implementation="flash_attention_2")

# from scripts.qwen3_vl_embedding import Qwen3VLEmbedder
import numpy as np
import torch

# Define a list of query texts
queries = [
    {"text": "A woman playing with her dog on a beach at sunset."},
    {"text": "Pet owner training dog outdoors near water."},
    {"text": "Woman surfing on waves during a sunny day."},
    {"text": "City skyline view from a high-rise building at night."}
]

# Define a list of document texts and images
documents = [
    {"text": "A woman shares a joyful moment with her golden retriever on a sun-drenched beach at sunset, as the dog offers its paw in a heartwarming display of companionship and trust."},
    {"image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg"},
    {"text": "A woman shares a joyful moment with her golden retriever on a sun-drenched beach at sunset, as the dog offers its paw in a heartwarming display of companionship and trust.", "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg"}
]

# Combine queries and documents into a single input list
inputs = queries + documents

# Process the inputs to get embeddings
embeddings = embedder.process(inputs)

# Compute similarity scores between query embeddings and document embeddings
similarity_scores = (embeddings[:4] @ embeddings[4:].T)

# Print out the similarity scores in a list format
print(similarity_scores.tolist())









"""---
## üé™ Part 4: Prebaked Demos

Let's explore the model's capabilities with several pre-built examples!

### üêï Demo 1: Image-Text Retrieval

Given text queries, find the most relevant images from a collection.
"""

# Sample images from the web
demo_images = {
    "beach_dog": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",
    "cat_laptop": "https://images.unsplash.com/photo-1526336024174-e58f5cdd8e13?w=400",
    "mountain": "https://images.unsplash.com/photo-1464822759023-fed622ff2c3b?w=400",
    "city_night": "https://images.unsplash.com/photo-1519681393784-d120267933ba?w=400",
    "food_pizza": "https://images.unsplash.com/photo-1565299624946-b28f40a0ae38?w=400",
}

# Display the images
from IPython.display import display, HTML
import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 5, figsize=(15, 3))
for ax, (name, url) in zip(axes, demo_images.items()):
    try:
        img = Image.open(BytesIO(requests.get(url, timeout=10).content))
        ax.imshow(img)
        ax.set_title(name, fontsize=10)
        ax.axis('off')
    except:
        ax.text(0.5, 0.5, 'Failed to load', ha='center', va='center')
        ax.axis('off')
plt.suptitle("üì∑ Demo Image Collection", fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

# Create embeddings for images
print("üîÑ Generating image embeddings...")
image_inputs = [{"image": url} for url in demo_images.values()]
image_embeddings = embedder.encode(image_inputs)
print(f"‚úÖ Generated {len(image_embeddings)} image embeddings of dimension {image_embeddings.shape[1]}")

# Store image names for retrieval
image_names = list(demo_images.keys())

# Define text queries
queries = [
    "A woman playing with her dog on a beach at sunset",
    "A fluffy cat relaxing",
    "Snowy mountain peaks",
    "City skyline at night with lights",
    "Delicious Italian food"
]

# Create query embeddings
print("üîÑ Generating query embeddings...")
query_inputs = [
    {"text": q, "instruction": "Retrieve images that match this description."}
    for q in queries
]
query_embeddings = embedder.encode(query_inputs)

# Compute similarities
similarities = embedder.similarity(query_embeddings, image_embeddings)

# Display results
print("\n" + "="*70)
print("üîç TEXT ‚Üí IMAGE RETRIEVAL RESULTS")
print("="*70)

for i, query in enumerate(queries):
    print(f"\nüìù Query: \"{query}\"")

    # Get ranked results
    ranked_indices = np.argsort(similarities[i])[::-1]

    print("   Top matches:")
    for rank, idx in enumerate(ranked_indices[:3]):
        score = similarities[i][idx]
        print(f"   {rank+1}. {image_names[idx]:15} (score: {score:.4f})")

"""### üìä Demo 2: Similarity Matrix Visualization

Let's visualize how queries relate to images with a heatmap.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Create heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(
    similarities,
    xticklabels=image_names,
    yticklabels=[q[:30] + "..." if len(q) > 30 else q for q in queries],
    annot=True,
    fmt='.3f',
    cmap='RdYlGn',
    vmin=0,
    vmax=1
)
plt.title('üéØ Query-Image Similarity Matrix', fontsize=14, fontweight='bold')
plt.xlabel('Images', fontsize=12)
plt.ylabel('Queries', fontsize=12)
plt.tight_layout()
plt.show()

"""### üîÄ Demo 3: Cross-Modal Comparison

Compare text descriptions with images and mixed text+image inputs.
"""

# Create mixed inputs
mixed_inputs = [
    # Text only
    {"text": "A golden retriever dog on a sunny beach with its owner"},
    # Image only (the beach dog image)
    {"image": demo_images["beach_dog"]},
    # Text + Image combined
    {
        "text": "A heartwarming moment of companionship",
        "image": demo_images["beach_dog"]
    },
    # A different text description
    {"text": "A cat sitting on a computer keyboard"},
]

input_labels = [
    "Text: Dog on beach",
    "Image: beach_dog",
    "Text+Image: Combined",
    "Text: Cat on keyboard"
]

print("üîÑ Generating mixed embeddings...")
mixed_embeddings = embedder.encode(mixed_inputs)

# Compute pairwise similarities
pairwise_sim = embedder.similarity(mixed_embeddings, mixed_embeddings)

# Visualize
plt.figure(figsize=(8, 6))
sns.heatmap(
    pairwise_sim,
    xticklabels=input_labels,
    yticklabels=input_labels,
    annot=True,
    fmt='.3f',
    cmap='Blues',
    vmin=0,
    vmax=1
)
plt.title('üîÄ Cross-Modal Similarity Matrix', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

print("\nüí° Notice how:")
print("   ‚Ä¢ Text description and matching image have high similarity")
print("   ‚Ä¢ Combined text+image creates an even richer representation")
print("   ‚Ä¢ Unrelated content (cat vs dog) has lower similarity")

"""---
## üóÉÔ∏è Part 5: Mini Multimodal RAG System

Now let's build a simple but powerful Multimodal RAG system!
"""

class MultimodalRAG:
    """
    A simple multimodal RAG system using Qwen3-VL-Embedding.

    Supports:
    - Adding text documents
    - Adding images (with optional captions)
    - Adding mixed text+image documents
    - Semantic search with any modality
    """

    def __init__(self, embedder: Qwen3VLEmbedder):
        self.embedder = embedder
        self.documents = []  # List of document dicts
        self.embeddings = None  # numpy array of embeddings

    def add_text(self, text: str, metadata: dict = None):
        """Add a text document."""
        doc = {
            "type": "text",
            "content": {"text": text},
            "display": text[:100] + "..." if len(text) > 100 else text,
            "metadata": metadata or {}
        }
        self.documents.append(doc)
        return len(self.documents) - 1

    def add_image(self, image: Union[str, Image.Image], caption: str = None, metadata: dict = None):
        """Add an image with optional caption."""
        content = {"image": image}
        if caption:
            content["text"] = caption

        doc = {
            "type": "image" if not caption else "image+text",
            "content": content,
            "display": caption if caption else f"Image: {str(image)[:50]}...",
            "metadata": metadata or {},
            "image_source": image if isinstance(image, str) else "PIL Image"
        }
        self.documents.append(doc)
        return len(self.documents) - 1

    def build_index(self):
        """Build the embedding index for all documents."""
        if not self.documents:
            raise ValueError("No documents to index!")

        print(f"üìä Building index for {len(self.documents)} documents...")
        inputs = [doc["content"] for doc in self.documents]
        self.embeddings = self.embedder.encode(inputs)
        print(f"‚úÖ Index built! Shape: {self.embeddings.shape}")

    def search(
        self,
        query: Union[str, Dict],
        top_k: int = 5,
        instruction: str = "Find relevant documents for this query."
    ) -> List[Dict]:
        """
        Search for relevant documents.

        Args:
            query: Text string or dict with 'text' and/or 'image'
            top_k: Number of results to return
            instruction: Search instruction

        Returns:
            List of (document, score) tuples
        """
        if self.embeddings is None:
            raise ValueError("Index not built! Call build_index() first.")

        # Prepare query
        if isinstance(query, str):
            query_input = {"text": query, "instruction": instruction}
        else:
            query_input = {**query, "instruction": instruction}

        # Get query embedding
        query_embedding = self.embedder.encode([query_input])

        # Compute similarities
        similarities = self.embedder.similarity(query_embedding, self.embeddings)[0]

        # Get top-k results
        top_indices = np.argsort(similarities)[::-1][:top_k]

        results = []
        for idx in top_indices:
            results.append({
                "document": self.documents[idx],
                "score": float(similarities[idx]),
                "index": int(idx)
            })

        return results

    def display_results(self, results: List[Dict], show_images: bool = True):
        """Pretty print search results."""
        print("\n" + "="*60)
        print("üîç SEARCH RESULTS")
        print("="*60)

        for i, result in enumerate(results):
            doc = result["document"]
            score = result["score"]

            print(f"\n{'‚îÄ'*60}")
            print(f"üìå Result {i+1} | Score: {score:.4f} | Type: {doc['type']}")
            print(f"{'‚îÄ'*60}")
            print(f"üìù {doc['display']}")

            if doc['metadata']:
                print(f"üè∑Ô∏è Metadata: {doc['metadata']}")

            # Show image if applicable
            if show_images and 'image' in doc['type'] and 'image_source' in doc:
                try:
                    source = doc['image_source']
                    if isinstance(source, str) and source.startswith('http'):
                        img = Image.open(BytesIO(requests.get(source, timeout=10).content))
                        img.thumbnail((200, 200))
                        display(img)
                except:
                    print("   [Could not display image]")

"""### üéØ Demo 4: Build and Query the RAG System"""

# Initialize RAG system
rag = MultimodalRAG(embedder)

# Add some text documents
rag.add_text(
    "Machine learning is a subset of artificial intelligence that enables systems to learn from data.",
    metadata={"topic": "AI", "source": "textbook"}
)
rag.add_text(
    "The golden retriever is one of the most popular dog breeds, known for their friendly nature.",
    metadata={"topic": "animals", "source": "encyclopedia"}
)
rag.add_text(
    "Pizza originated in Naples, Italy, and has become one of the world's most beloved foods.",
    metadata={"topic": "food", "source": "food blog"}
)
rag.add_text(
    "Mount Everest is the tallest mountain in the world, located in the Himalayas.",
    metadata={"topic": "geography", "source": "atlas"}
)

# Add images with captions
rag.add_image(
    demo_images["beach_dog"],
    caption="A woman enjoying time with her golden retriever on a beautiful beach at sunset",
    metadata={"topic": "pets", "location": "beach"}
)
rag.add_image(
    demo_images["mountain"],
    caption="Majestic snow-capped mountain peaks under a clear blue sky",
    metadata={"topic": "nature", "location": "mountains"}
)
rag.add_image(
    demo_images["food_pizza"],
    caption="Delicious homemade pizza with fresh toppings",
    metadata={"topic": "food", "cuisine": "Italian"}
)
rag.add_image(
    demo_images["city_night"],
    caption="City skyline illuminated at night with mountain backdrop",
    metadata={"topic": "urban", "time": "night"}
)

print(f"\nüìö Added {len(rag.documents)} documents to the RAG system")

# Build the index
rag.build_index()

# Text query
print("\nüîé Query: 'Tell me about dogs and pets'")
results = rag.search("Tell me about dogs and pets", top_k=3)
rag.display_results(results)

# Food-related query
print("\nüîé Query: 'Italian cuisine and traditional recipes'")
results = rag.search("Italian cuisine and traditional recipes", top_k=3)
rag.display_results(results)

# Nature query
print("\nüîé Query: 'Tall mountains and hiking destinations'")
results = rag.search("Tall mountains and hiking destinations", top_k=3)
rag.display_results(results)

"""---
## üì§ Part 6: Upload Your Own Images!

Now it's your turn! Upload your own images and add them to the RAG system.
"""

from google.colab import files
import os

def upload_and_add_images():
    """Upload images and add them to the RAG system."""
    print("üì§ Upload your images (JPG, PNG, etc.)")
    print("   After uploading, you'll be prompted to add captions.\n")

    uploaded = files.upload()

    if not uploaded:
        print("No files uploaded.")
        return []

    added_indices = []

    for filename, content in uploaded.items():
        try:
            # Load image
            img = Image.open(BytesIO(content)).convert('RGB')

            # Display thumbnail
            display_img = img.copy()
            display_img.thumbnail((300, 300))
            print(f"\nüì∑ Uploaded: {filename}")
            display(display_img)

            # Get caption from user
            caption = input(f"Enter a caption for '{filename}' (or press Enter to skip): ").strip()

            # Add to RAG
            idx = rag.add_image(
                img,
                caption=caption if caption else None,
                metadata={"filename": filename, "source": "user_upload"}
            )
            added_indices.append(idx)
            print(f"‚úÖ Added to RAG with index {idx}")

        except Exception as e:
            print(f"‚ùå Error processing {filename}: {e}")

    if added_indices:
        print(f"\nüîÑ Rebuilding index with {len(rag.documents)} total documents...")
        rag.build_index()
        print("‚úÖ Index rebuilt!")

    return added_indices

# Uncomment and run this cell to upload your images!
# uploaded_indices = upload_and_add_images()

def interactive_search():
    """Interactive search interface."""
    print("\n" + "="*60)
    print("üîç INTERACTIVE MULTIMODAL SEARCH")
    print("="*60)
    print("Type your queries below. Enter 'quit' to exit.\n")

    while True:
        query = input("üîé Your query: ").strip()

        if query.lower() in ['quit', 'exit', 'q']:
            print("üëã Goodbye!")
            break

        if not query:
            continue

        results = rag.search(query, top_k=3)
        rag.display_results(results, show_images=True)
        print("\n")

# Uncomment to start interactive search!
# interactive_search()

"""### üåê Alternative: Add Images via URL

If you prefer, you can add images directly via URL:
"""

def add_image_from_url(url: str, caption: str = None):
    """Add an image from URL to the RAG system."""
    try:
        # Test loading the image
        response = requests.get(url, timeout=10)
        img = Image.open(BytesIO(response.content))

        # Display
        display_img = img.copy()
        display_img.thumbnail((300, 300))
        display(display_img)

        # Add to RAG
        idx = rag.add_image(
            url,
            caption=caption,
            metadata={"source": "url", "url": url}
        )

        print(f"‚úÖ Added image from URL (index: {idx})")
        print("üîÑ Don't forget to rebuild the index with: rag.build_index()")
        return idx

    except Exception as e:
        print(f"‚ùå Error: {e}")
        return None

# Example usage:
# add_image_from_url(
#     "https://your-image-url.com/image.jpg",
#     caption="Description of the image"
# )
# rag.build_index()  # Rebuild after adding

"""---
## üöÄ Part 7: Advanced Features

### Image-to-Image Search
Search for similar images using an image as the query!
"""


# Image-to-Image search
print("üñºÔ∏è IMAGE-TO-IMAGE SEARCH")
print("Using the beach dog image as query...\n")

# Display query image
query_img = Image.open("/content/Golden_Retriever.jpg")
# query_img_url =  demo_images["beach_dog"]
# query_img = Image.open(BytesIO(requests.get(query_img_url, timeout=10).content))
query_img.thumbnail((200, 200))
print("Query Image:")
display(query_img)

# Search using image
results = rag.search(
    {"image": query_img_url},
    top_k=3,
    instruction="Find documents similar to this image."
)

rag.display_results(results)

"""### Embedding Dimension Reduction (MRL)

The model supports Matryoshka Representation Learning - you can use lower dimensions for faster search!
"""

def reduce_embedding_dim(embeddings: np.ndarray, target_dim: int) -> np.ndarray:
    """
    Reduce embedding dimensions using MRL (just take first N dimensions).
    Valid dimensions: 64, 128, 256, 512, 1024, 2048
    """
    reduced = embeddings[:, :target_dim]
    # Re-normalize
    norms = np.linalg.norm(reduced, axis=1, keepdims=True)
    return reduced / norms

# Compare different dimensions
print("üìä EMBEDDING DIMENSION COMPARISON")
print("="*60)

# Get embeddings
test_inputs = [
    {"text": "A dog playing on the beach"},
    {"image": demo_images["beach_dog"]},
]
full_embeddings = embedder.encode(test_inputs)

dimensions = [64, 128, 256, 512, 1024, 2048]
print(f"\nFull dimension similarity (2048): {full_embeddings[0] @ full_embeddings[1]:.4f}")
print("\nReduced dimensions:")

for dim in dimensions:
    reduced = reduce_embedding_dim(full_embeddings, dim)
    sim = reduced[0] @ reduced[1]
    print(f"  {dim:>4}d: {sim:.4f}")

